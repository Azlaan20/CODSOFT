{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Facial Detection and Recognition Project** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing the libraries we will need for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use pre-trained models as train a facial detection model is time consuming, requires a lot of data and a lot of computing power. We will use the pre-trained models from the OpenCV library. We will use the Haar Cascade Classifier for the face detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained classification models\n",
    "emotion_model = load_model('emotionModel.hdf5')\n",
    "gender_model = load_model('genderModel.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the stored emotions and genders so that they can be easily understood by the user/reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of class indices to human-readable labels\n",
    "emotion_labels = [\"Angry\",\"Disgust\",\"Fear\", \"Happy\", \"Surprise\", \"Neutral\"]\n",
    "gender_labels = [\"Male\", \"Female\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define a function that will detect the face in the image and identify the associated emotions and gender of the person in question. We will use the Haar Cascade Classifier for the face detection. The function also resizes the input images based upon the requirements of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_classify_faces(input_pil_image):\n",
    "\n",
    "    if isinstance(input_pil_image, Image.Image):\n",
    "        input_array = np.array(input_pil_image)\n",
    "    else:\n",
    "        input_array = input_pil_image\n",
    "\n",
    "\n",
    "    if input_array.shape[2] == 4:\n",
    "        input_array = cv2.cvtColor(input_array, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "    gray = cv2.cvtColor(input_array, cv2.COLOR_RGB2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    \n",
    "    print(\"Number of detected faces:\", len(faces))\n",
    "\n",
    "\n",
    "    output_array = input_array.copy()\n",
    "\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = input_array[y:y + h, x:x + w]\n",
    "\n",
    "         # Resize face for gender classification\n",
    "        resized_face_gender = cv2.resize(face, (150, 150))\n",
    "        gender_idx = np.argmax(gender_model.predict(np.expand_dims(resized_face_gender, axis=0)))\n",
    "        predicted_gender = gender_labels[gender_idx]\n",
    "\n",
    "        # Resize face and convert to grayscale for emotion classification\n",
    "        resized_face_emotion = cv2.resize(face, (64, 64))\n",
    "        gray_face_emotion = cv2.cvtColor(resized_face_emotion, cv2.COLOR_RGB2GRAY)\n",
    "        gray_face_emotion = gray_face_emotion.reshape(64, 64, 1)\n",
    "        emotion_idx = np.argmax(emotion_model.predict(np.expand_dims(gray_face_emotion, axis=0)))\n",
    "        predicted_emotion = emotion_labels[emotion_idx]\n",
    "\n",
    "        cv2.rectangle(output_array, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(output_array, f\"Gender: {predicted_gender}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        cv2.putText(output_array, f\"Emotion: {predicted_emotion}\", (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return Image.fromarray(output_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have created the model. It is only reasonable to use a GUI so that we might make it user-friendly and accesible. For this, I have used the gradios library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azlaan\\AppData\\Local\\Temp\\ipykernel_11248\\991791201.py:3: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  inputs=gr.inputs.Image(type=\"pil\", label=\"Upload an image or video file\"),\n",
      "C:\\Users\\Azlaan\\AppData\\Local\\Temp\\ipykernel_11248\\991791201.py:3: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  inputs=gr.inputs.Image(type=\"pil\", label=\"Upload an image or video file\"),\n",
      "C:\\Users\\Azlaan\\AppData\\Local\\Temp\\ipykernel_11248\\991791201.py:4: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  outputs=gr.outputs.Image(type=\"pil\")\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=detect_and_classify_faces,\n",
    "    inputs=gr.inputs.Image(type=\"pil\", label=\"Upload an image or video file\"),\n",
    "    outputs=gr.outputs.Image(type=\"pil\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected faces: 1\n",
      "1/1 [==============================] - 1s 908ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n"
     ]
    }
   ],
   "source": [
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
